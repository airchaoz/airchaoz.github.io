<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>TF Lite和Flex 的编译、使用 - echo&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="echo"><meta name="msapplication-TileImage" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202408101608074.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="echo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="当谈到深度学习和机器学习框架时，必定绕不开Tensorflow。作为一个备受欢迎的开源工具，它被广泛用于构建、训练和部署机器学习模型。TensorFlow 由 Google 开发，并于2015年首次发布，它的目标是提供一个灵活、可扩展且易于使用的框架，使研究人员和工程师能够快速开发和部署深度学习模型。"><meta property="og:type" content="blog"><meta property="og:title" content="TF Lite和Flex 的编译、使用"><meta property="og:url" content="http://www.airchaoz.top/2023/12/13/TF-Lite%E5%92%8CFlex-%E7%9A%84%E7%BC%96%E8%AF%91%E3%80%81%E4%BD%BF%E7%94%A8/"><meta property="og:site_name" content="echo&#039;s blog"><meta property="og:description" content="当谈到深度学习和机器学习框架时，必定绕不开Tensorflow。作为一个备受欢迎的开源工具，它被广泛用于构建、训练和部署机器学习模型。TensorFlow 由 Google 开发，并于2015年首次发布，它的目标是提供一个灵活、可扩展且易于使用的框架，使研究人员和工程师能够快速开发和部署深度学习模型。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131559459.png"><meta property="og:image" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601190.png"><meta property="og:image" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601432.png"><meta property="og:image" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202403262257900.png"><meta property="article:published_time" content="2023-12-13T13:55:01.000Z"><meta property="article:modified_time" content="2024-08-10T14:08:20.597Z"><meta property="article:author" content="echo"><meta property="article:tag" content="机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131559459.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.airchaoz.top/2023/12/13/TF-Lite%E5%92%8CFlex-%E7%9A%84%E7%BC%96%E8%AF%91%E3%80%81%E4%BD%BF%E7%94%A8/"},"headline":"TF Lite和Flex 的编译、使用","image":["https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131559459.png","https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601190.png","https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601432.png","https://blog-512.oss-cn-shenzhen.aliyuncs.com/202403262257900.png"],"datePublished":"2023-12-13T13:55:01.000Z","dateModified":"2024-08-10T14:08:20.597Z","author":{"@type":"Person","name":"echo"},"publisher":{"@type":"Organization","name":"echo's blog","logo":{"@type":"ImageObject","url":{"text":"echo's blog"}}},"description":"当谈到深度学习和机器学习框架时，必定绕不开Tensorflow。作为一个备受欢迎的开源工具，它被广泛用于构建、训练和部署机器学习模型。TensorFlow 由 Google 开发，并于2015年首次发布，它的目标是提供一个灵活、可扩展且易于使用的框架，使研究人员和工程师能够快速开发和部署深度学习模型。"}</script><link rel="canonical" href="http://www.airchaoz.top/2023/12/13/TF-Lite%E5%92%8CFlex-%E7%9A%84%E7%BC%96%E8%AF%91%E3%80%81%E4%BD%BF%E7%94%A8/"><link rel="icon" href="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202408101608074.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-9N9JB3XQG1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-9N9JB3XQG1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">echo&#039;s blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/airchaoz"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-12-13T13:55:01.000Z" title="12/13/2023, 9:55:01 PM">2023-12-13</time>发表</span><span class="level-item"><time dateTime="2024-08-10T14:08:20.597Z" title="8/10/2024, 10:08:20 PM">2024-08-10</time>更新</span><span class="level-item">16 分钟读完 (大约2436个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">TF Lite和Flex 的编译、使用</h1><div class="content"><p>当谈到深度学习和机器学习框架时，必定绕不开Tensorflow。作为一个备受欢迎的开源工具，它被广泛用于构建、训练和部署机器学习模型。TensorFlow 由 Google 开发，并于2015年首次发布，它的目标是提供一个灵活、可扩展且易于使用的框架，使研究人员和工程师能够快速开发和部署深度学习模型。</p>
<span id="more"></span>

<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131559459.png"></p>
<p>对于移动和嵌入式设备，TensorFlow 提供了 TensorFlow Lite。它是 TensorFlow 的一个轻量级版本，专门设计用于在移动设备、物联网设备和嵌入式系统等资源受限的环境中运行机器学习模型。它的目标是在保持模型性能的同时，提供更小的模型尺寸和更快的推理速度，以满足移动和嵌入式应用的需求。以下是 TensorFlow Lite 的一些关键特点和用途：</p>
<ol>
<li><strong>轻量级</strong>：TFLite 通过使用量化技术、模型剪枝和模型蒸馏等方法，将深度学习模型的大小大幅减小，以适应资源有限的设备。这有助于减少模型的存储空间和内存占用。</li>
<li><strong>快速推理</strong>：TFLite 针对嵌入式设备进行了优化，以实现快速的推理速度。这对于实时应用和响应时间敏感的任务非常重要，如图像识别、语音识别和姿态估计等。</li>
<li><strong>硬件加速支持</strong>：TFLite 支持多种硬件加速器，包括GPU、TPU和边缘设备上的专用加速器。这允许模型在加速硬件上运行，进一步提高了推理速度。</li>
<li><strong>跨平台兼容性</strong>：TFLite 可以在多个操作系统上运行，包括Android、iOS、Linux和嵌入式操作系统，使其适用于各种移动和嵌入式平台。</li>
<li><strong>易于集成</strong>：TFLite 提供了针对多种编程语言的 API，包括Python、C++和Java，使开发者能够轻松集成模型到他们的应用中。</li>
<li><strong>量化和转换工具</strong>：TFLite 提供了用于将训练好的 TensorFlow 模型转换为 TFLite 格式的工具，同时还支持量化，这有助于减小模型的尺寸并提高性能。</li>
<li><strong>模型兼容性</strong>：TFLite 支持多种类型的模型，包括图像分类、目标检测、自然语言处理和语音处理等。这使得开发者能够在移动和嵌入式设备上部署各种机器学习任务。</li>
</ol>
<h1 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h1><p>TFLite是Tensorflow的子模块，源码也放在Tensorflow当中，可以直接从Github上下载。Git使用如下命令克隆到本地：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/tensorflow.git</span><br></pre></td></tr></table></figure>

<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601190.png"></p>
<h1 id="程序编译"><a href="#程序编译" class="headerlink" title="程序编译"></a>程序编译</h1><p>Tensorflow的编译不是一件简单的事情，本人踩了很多坑，这里记录一下编译过程，避免后人踩坑。</p>
<p>Tensorflow提供了CMake和Bazel两种编译途径，<strong>这里强烈推荐使用Bazel编译</strong>，这也是官方推荐。CMake虽然有时也能编译成功，但是维护程度没有Bazel好，更容易编译失败。</p>
<aside>
💡 Tensorflow的版本跟Bazel是存在版本对应关系的，官方只会测试少部分指定的版本。

</aside>

<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401131601432.png"></p>
<p>这里强烈建议使用Bazelisk，它能够自动识别编译所需要的Bazel并自动下载切换到对应的版本。使用方法也很简单，从Github上下载Release，重命名成bazel(Windows下是bazel.exe)并添加到环境变量当中即可。</p>
<h2 id="Windows平台编译"><a href="#Windows平台编译" class="headerlink" title="Windows平台编译"></a>Windows平台编译</h2><h3 id="环境依赖："><a href="#环境依赖：" class="headerlink" title="环境依赖："></a>环境依赖：</h3><ul>
<li>VC2019：在WIN平台编译需要准备VC编译环境，这里本人使用VC2019，因为Tensorflow使用的C++标准库比较新，使用旧版本VC可能会无法编译。</li>
<li>Git Bash：编译时需要Bash来执行某些脚本，因为Win本身不支持bash，所以需要借助Git Bash</li>
<li>Python+Numpy模块</li>
</ul>
<h3 id="Lite编译步骤："><a href="#Lite编译步骤：" class="headerlink" title="Lite编译步骤："></a>Lite编译步骤：</h3><aside>
💡
**一定要使用CMD！一定要使用CMD！一定要使用CMD！不要使用PowerShell**

</aside>

<p>进入到源码目录，先生成编译配置，选项看自己需要来选。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python .\configure.py</span><br></pre></td></tr></table></figure>

<p>在编译之前设置好编译变量，建议设置好Git Bash路径和VC路径，避免编译找不到这两个。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> BAZEL_SH=D:/Program Files/Git/bin/bash.exe</span><br><span class="line"><span class="built_in">set</span> BAZEL_VC=D:\\Program Files (x86)\\Microsoft Visual Studio\\2019\BuildTools\\VC</span><br></pre></td></tr></table></figure>

<p>编译需要能上外网，否则很多源码无法下载。</p>
<p>使用以下命令进行编译，-c opt表示编译Release版本，有需要可以改成DEBUG版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build -c opt --config=mkl //tensorflow/lite:tensorflowlite</span><br></pre></td></tr></table></figure>

<p>编译好之后在<code>\bazel-out\x64_windows-opt\bin\tensorflow\lite</code>文件下可以找到<code>tensorflowlite.dll</code>和<code>tensorflowlite.dll.if.lib</code>，这就是我们需要的tensorflow动态库文件。</p>
<h3 id="FLex编译步骤："><a href="#FLex编译步骤：" class="headerlink" title="FLex编译步骤："></a>FLex编译步骤：</h3><p>并不是所有的模型都能转Lite格式，默认的Lite只支持少量的算子，如果想要复杂的算子需要使用Flex委托，官方关于这部分的解释如下</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202403262257900.png" alt="Untitled"></p>
<p>Flex的编译也是只需要一行命令，但是需要注意的是，编译Flex需要编译Tensorflow的核心库，需要非常久的时间，建议有条件放到多核编译机上编译。</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build <span class="literal">-c</span> opt <span class="literal">--config</span>=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex</span><br></pre></td></tr></table></figure>

<p>编译完成后可以在<code>\bazel-out\x64_windows-opt\bin\tensorflow\lite\delegates\flex</code> 目录里找到<code>tensorflowlite_flex.dll</code>和<code>tensorflowlite_flex.dll.if.lib</code></p>
<h2 id="Linux平台编译"><a href="#Linux平台编译" class="headerlink" title="Linux平台编译"></a>Linux平台编译</h2><h3 id="环境依赖：-1"><a href="#环境依赖：-1" class="headerlink" title="环境依赖："></a>环境依赖：</h3><ul>
<li>GCC：建议使用高版本的GCC进行编译，对C++14、17支持的版本最佳</li>
<li>Python+Numpy模块</li>
</ul>
<h3 id="编译步骤："><a href="#编译步骤：" class="headerlink" title="编译步骤："></a>编译步骤：</h3><p>Linux环境下的编译比较简单，先生成编译配置，再编译</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python .\configure.py</span><br><span class="line">bazel build <span class="literal">-c</span> opt <span class="literal">--config</span>=mkl //tensorflow/lite:tensorflowlite</span><br><span class="line">bazel build <span class="literal">-c</span> opt <span class="literal">--config</span>=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex</span><br></pre></td></tr></table></figure>

<h1 id="程序调用"><a href="#程序调用" class="headerlink" title="程序调用"></a>程序调用</h1><p>调用需要用到Tensorflow Lite C++ API，通过文档可以查询到api接口及其功能。</p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/lite/api_docs/cc">TensorFlow Lite C++ API Reference</a></p>
<h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a><strong>头文件</strong></h2><p>调用需要用到<code>tensorflow</code>和<code>flatbuffers</code>的头文件，<code>tensorflow</code>就使用源码文件夹，<code>flatbuffers</code>的头文件在路径<code>bazel-out\x64_windows-opt\bin\external\flatbuffers\_virtual_includes\flatbuffers</code>中可以找到，Linux在类似的目录。</p>
<p>调用主要使用这3个头文件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/interpreter.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/kernels/register.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/model.h&quot;</span></span></span><br></pre></td></tr></table></figure>

<h2 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a><strong>加载模型</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::unique_ptr&lt;tflite::FlatBufferModel&gt; model =</span><br><span class="line">    tflite::FlatBufferModel::<span class="built_in">BuildFromFile</span>(<span class="string">&quot;../model/lstm_1mon_5s.tflite&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="创建解释器"><a href="#创建解释器" class="headerlink" title="创建解释器"></a><strong>创建解释器</strong></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> tflite::ops::builtin::BuiltinOpResolver resolver;</span><br><span class="line"> std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;</span><br><span class="line"> tflite::<span class="built_in">InterpreterBuilder</span>(*model, resolver)(&amp;interpreter);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分配解释器内存</span></span><br><span class="line"> <span class="keyword">if</span> (interpreter-&gt;<span class="built_in">AllocateTensors</span>() != kTfLiteOk) &#123;</span><br><span class="line">   std::cerr &lt;&lt; <span class="string">&quot;Failed to allocate tensors.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h2 id="输出模型信息"><a href="#输出模型信息" class="headerlink" title="输出模型信息"></a>输出模型信息</h2><p>tflite模型文件包含了推理所需要的尺寸信息，可以将其输出出来便于错误检查。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 获取输入输出张量</span></span><br><span class="line"> TfLiteTensor* input_tensor = interpreter-&gt;<span class="built_in">input_tensor</span>(<span class="number">0</span>);</span><br><span class="line"> TfLiteTensor* output_tensor = interpreter-&gt;<span class="built_in">output_tensor</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入数据尺寸</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> input_tensor_size = input_tensor-&gt;bytes / <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br></pre></td></tr></table></figure>

<h2 id="填充推理数据"><a href="#填充推理数据" class="headerlink" title="填充推理数据"></a>填充推理数据</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* input_data = interpreter-&gt;<span class="built_in">typed_input_tensor</span>&lt;<span class="type">float</span>&gt;(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">	input_data[i] = <span class="number">0.5f</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="运行模型推理"><a href="#运行模型推理" class="headerlink" title="运行模型推理"></a>运行模型推理</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 运行模型</span></span><br><span class="line"><span class="keyword">if</span> (interpreter-&gt;<span class="built_in">Invoke</span>() != kTfLiteOk) &#123;</span><br><span class="line">  std::cerr &lt;&lt; <span class="string">&quot;Failed to invoke interpreter.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="获取输出数据"><a href="#获取输出数据" class="headerlink" title="获取输出数据"></a>获取输出数据</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取输出数据</span></span><br><span class="line"><span class="type">float</span>* output_data = interpreter-&gt;<span class="built_in">typed_output_tensor</span>&lt;<span class="type">float</span>&gt;(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>

<p>完整的Demo如下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/interpreter.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/kernels/register.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tensorflow/lite/model.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">loadModelAndRunInference</span><span class="params">(<span class="type">const</span> std::string model_path,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">const</span> <span class="type">int</span> time_step,</span></span></span><br><span class="line"><span class="params"><span class="function">                              std::vector&lt;std::vector&lt;<span class="type">float</span>&gt;&gt; &amp;indata,</span></span></span><br><span class="line"><span class="params"><span class="function">                              std::vector&lt;<span class="type">float</span>&gt; &amp;outdata)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!indata.<span class="built_in">size</span>()) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;输入数据不能为空&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (indata[<span class="number">0</span>].<span class="built_in">size</span>() &lt; time_step) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;输入数据量小于样本时间步长&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 加载模型</span></span><br><span class="line">  <span class="keyword">auto</span> model = tflite::FlatBufferModel::<span class="built_in">BuildFromFile</span>(model_path.<span class="built_in">c_str</span>());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!model) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Failed to load model.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建解释器</span></span><br><span class="line">  tflite::ops::builtin::BuiltinOpResolver resolver;</span><br><span class="line">  std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;</span><br><span class="line">  tflite::<span class="built_in">InterpreterBuilder</span>(*model, resolver)(&amp;interpreter);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!interpreter) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Failed to construct interpreter.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分配张量</span></span><br><span class="line">  <span class="keyword">if</span> (interpreter-&gt;<span class="built_in">AllocateTensors</span>() != kTfLiteOk) &#123;</span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Failed to allocate tensors.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取输入输出张量</span></span><br><span class="line">  TfLiteTensor* input_tensor = interpreter-&gt;<span class="built_in">input_tensor</span>(<span class="number">0</span>);</span><br><span class="line">  TfLiteTensor* output_tensor = interpreter-&gt;<span class="built_in">output_tensor</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 填充输入数据</span></span><br><span class="line">  <span class="type">int</span> input_tensor_size = input_tensor-&gt;bytes / <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;输入数据大小为:&quot;</span> &lt;&lt; input_tensor_size &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> vec_num = indata.<span class="built_in">size</span>();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; indata[<span class="number">0</span>].<span class="built_in">size</span>() - time_step; n++) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* input_data = interpreter-&gt;<span class="built_in">typed_input_tensor</span>&lt;<span class="type">float</span>&gt;(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; time_step; ++i) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; vec_num; ++j) &#123;</span><br><span class="line">        input_data[i*vec_num+j] = indata[j][n+i];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 运行模型</span></span><br><span class="line">    <span class="keyword">if</span> (interpreter-&gt;<span class="built_in">Invoke</span>() != kTfLiteOk) &#123;</span><br><span class="line">      std::cerr &lt;&lt; <span class="string">&quot;Failed to invoke interpreter.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出数据</span></span><br><span class="line">    <span class="type">float</span>* output_data = interpreter-&gt;<span class="built_in">typed_output_tensor</span>&lt;<span class="type">float</span>&gt;(<span class="number">0</span>);</span><br><span class="line">    outdata.<span class="built_in">push_back</span>(*output_data);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Flex调用"><a href="#Flex调用" class="headerlink" title="Flex调用"></a>Flex调用</h1><p>官方申明，只要链接了共享库，在运行时创建解释器时，就会自动安装必要的 <code>TfLiteDelegate</code>。不需要像其他委托类型通常要求的那样显式安装委托实例。</p>
<p>在Linux平台中，只需要使用<code>--no-as-needed</code>来强制链接<code>tensorflowlite_flex.so</code>即可，CMake脚本如下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cmake_minimum_required</span>(VERSION <span class="number">3.11</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">project</span>(predict)</span><br><span class="line"><span class="built_in">set</span>(CMAKE_CXX_STANDARD <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">include_directories</span>(</span><br><span class="line">  /home/airchaoz/miniconda3/envs/tflite/lib/python3<span class="number">.8</span>/site-packages/tensorflow/include</span><br><span class="line">  /home/airchaoz/repository/tf_lite/library/lite/include</span><br><span class="line">  /home/airchaoz/repository/tf_lite/tensorflow</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">add_compile_options</span>(<span class="string">&quot;-g&quot;</span>)</span><br><span class="line"># 添加动态</span><br><span class="line"><span class="built_in">set</span>(LINK_DIR /home/airchaoz/repository/tf_lite/library/lite/lib/)</span><br><span class="line"><span class="built_in">add_executable</span>(predict src/main.cpp )</span><br><span class="line"></span><br><span class="line"><span class="built_in">link_libraries</span>($&#123;LINK_DIR&#125;/libflatbuffers.a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">target_link_libraries</span>(predict $&#123;LINK_DIR&#125;libtensorflowlite.so)</span><br><span class="line"># 强制链接整个库</span><br><span class="line"><span class="built_in">target_link_libraries</span>(predict</span><br><span class="line">  -Wl,--no-as-needed</span><br><span class="line">  $&#123;LINK_DIR&#125;libtensorflowlite_flex.so</span><br><span class="line">  -Wl,--as-needed</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>在Win平台中，找不到强制链接的选项，这里使用手动加载的方式实现</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>TF Lite和Flex 的编译、使用</p><p><a href="http://www.airchaoz.top/2023/12/13/TF-Lite和Flex-的编译、使用/">http://www.airchaoz.top/2023/12/13/TF-Lite和Flex-的编译、使用/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>echo</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2023-12-13</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-08-10</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/02/14/GDB-Dashboard%E7%9A%84%E4%BD%BF%E7%94%A8/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">GDB-Dashboard的使用</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/08/27/CMake%E8%BF%9B%E9%98%B6/"><span class="level-item">CMake进阶</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.airchaoz.top/2023/12/13/TF-Lite%E5%92%8CFlex-%E7%9A%84%E7%BC%96%E8%AF%91%E3%80%81%E4%BD%BF%E7%94%A8/';
            this.page.identifier = '2023/12/13/TF-Lite和Flex-的编译、使用/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'echos-blog-2' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401032048869.png" alt="echo"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">echo</p><p class="is-size-6 is-block">C++ Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>GuangDong, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">14</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CMake/"><span class="tag">CMake</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Qt/"><span class="tag">Qt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"><span class="tag">内存模型</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"><span class="tag">并发编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"><span class="tag">并行计算</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="tag">开发工具</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"><span class="tag">性能优化</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%86%85%E5%AD%98/"><span class="tag">操作系统, 内存</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C-%E6%8A%93%E5%8C%85/"><span class="tag">网络, 抓包</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="tag">网络编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"><span class="tag">跨平台开发</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#源码下载"><span class="level-left"><span class="level-item">1</span><span class="level-item">源码下载</span></span></a></li><li><a class="level is-mobile" href="#程序编译"><span class="level-left"><span class="level-item">2</span><span class="level-item">程序编译</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Windows平台编译"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Windows平台编译</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#环境依赖："><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">环境依赖：</span></span></a></li><li><a class="level is-mobile" href="#Lite编译步骤："><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">Lite编译步骤：</span></span></a></li><li><a class="level is-mobile" href="#FLex编译步骤："><span class="level-left"><span class="level-item">2.1.3</span><span class="level-item">FLex编译步骤：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Linux平台编译"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Linux平台编译</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#环境依赖：-1"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">环境依赖：</span></span></a></li><li><a class="level is-mobile" href="#编译步骤："><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">编译步骤：</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#程序调用"><span class="level-left"><span class="level-item">3</span><span class="level-item">程序调用</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#头文件"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">头文件</span></span></a></li><li><a class="level is-mobile" href="#加载模型"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">加载模型</span></span></a></li><li><a class="level is-mobile" href="#创建解释器"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">创建解释器</span></span></a></li><li><a class="level is-mobile" href="#输出模型信息"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">输出模型信息</span></span></a></li><li><a class="level is-mobile" href="#填充推理数据"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">填充推理数据</span></span></a></li><li><a class="level is-mobile" href="#运行模型推理"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">运行模型推理</span></span></a></li><li><a class="level is-mobile" href="#获取输出数据"><span class="level-left"><span class="level-item">3.7</span><span class="level-item">获取输出数据</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Flex调用"><span class="level-left"><span class="level-item">4</span><span class="level-item">Flex调用</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">echo&#039;s blog</a><p class="is-size-7"><span>&copy; 2024 echo</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>