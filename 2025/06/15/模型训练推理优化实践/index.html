<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>模型训练推理优化实践(一):训练优化 - echo&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="echo"><meta name="msapplication-TileImage" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202408101608074.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="echo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="NVIDIA提供了一些适用于CUDA层面的Profiling工具，其中最重要的是Nsight产品族的性能分析工具Nsight Systems、Nsight Compute和Nsight Graphics。"><meta property="og:type" content="blog"><meta property="og:title" content="模型训练推理优化实践(一):训练优化"><meta property="og:url" content="http://www.airchaoz.top/2025/06/15/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"><meta property="og:site_name" content="echo&#039;s blog"><meta property="og:description" content="NVIDIA提供了一些适用于CUDA层面的Profiling工具，其中最重要的是Nsight产品族的性能分析工具Nsight Systems、Nsight Compute和Nsight Graphics。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/ChatGPT%20Image%202025%E5%B9%B46%E6%9C%8815%E6%97%A5%2015_28_38.png"><meta property="article:published_time" content="2025-06-15T07:03:13.000Z"><meta property="article:modified_time" content="2025-06-15T08:07:44.781Z"><meta property="article:author" content="echo"><meta property="article:tag" content="机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://blog-512.oss-cn-shenzhen.aliyuncs.com/ChatGPT%20Image%202025%E5%B9%B46%E6%9C%8815%E6%97%A5%2015_28_38.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.airchaoz.top/2025/06/15/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"},"headline":"模型训练推理优化实践(一):训练优化","image":["https://blog-512.oss-cn-shenzhen.aliyuncs.com/ChatGPT%20Image%202025%E5%B9%B46%E6%9C%8815%E6%97%A5%2015_28_38.png"],"datePublished":"2025-06-15T07:03:13.000Z","dateModified":"2025-06-15T08:07:44.781Z","author":{"@type":"Person","name":"echo"},"publisher":{"@type":"Organization","name":"echo's blog","logo":{"@type":"ImageObject","url":{"text":"echo's blog"}}},"description":"NVIDIA提供了一些适用于CUDA层面的Profiling工具，其中最重要的是Nsight产品族的性能分析工具Nsight Systems、Nsight Compute和Nsight Graphics。"}</script><link rel="canonical" href="http://www.airchaoz.top/2025/06/15/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"><link rel="icon" href="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202408101608074.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-9N9JB3XQG1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-9N9JB3XQG1');</script><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="echo's blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">echo&#039;s blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/changelog">ChangeLog</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub Page" href="https://github.com/airchaoz"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Rss" href="/atom.xml"><i class="fas fa-rss"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/ChatGPT%20Image%202025%E5%B9%B46%E6%9C%8815%E6%97%A5%2015_28_38.png" alt="模型训练推理优化实践(一):训练优化"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-06-15T07:03:13.000Z" title="6/15/2025, 3:03:13 PM">2025-06-15</time>发表</span><span class="level-item"><time dateTime="2025-06-15T08:07:44.781Z" title="6/15/2025, 4:07:44 PM">2025-06-15</time>更新</span><span class="level-item">30 分钟读完 (大约4478个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">模型训练推理优化实践(一):训练优化</h1><div class="content"><p>NVIDIA提供了一些适用于CUDA层面的Profiling工具，其中最重要的是Nsight产品族的性能分析工具Nsight Systems、Nsight Compute和Nsight Graphics。</p>
<span id="more"></span>
<p>这里两个工具使用上互补，Nsight Systems更侧重整个系统性能分析，监视了CPU、GPU、内存和网络等资源的情况，可以识别系统整体的瓶颈。除了CUDA之外，也支持OpenGL、DirectX、Vulkan、CPU 线程等分析。</p>
<p>Nsight Graphics更适用于内核级微观分析，优化算子的计算效率。</p>
<p>例如，使用Nsight Systems和Nsight Compute对AI应用实现性能分析与优化的流程为：</p>
<ol>
<li>先利用Nsight Systems观察到某个CUDA Kernel具体运行时间的功能，分析一下程序。</li>
<li>如果发现某个Kernel运行时间过长，可以使用Nsight Compute对这个CUDA Kernel做进一步的性能分析并进行优化。</li>
<li>CUDA Kernel如果优化完成，可以再次使用Nsight Systems对程序做Profiling，如此反复，直到整个程序的性能优化达到预期结果。</li>
</ol>
<h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>该实践需要安装以下环境：</p>
<ul>
<li>pytorch(GPU版本):用于下载、加载和训练和推理模型</li>
<li>nvtx:在程序中插入标记和区间，更清晰地可视化代码执行过程，便于性能分析与调优。</li>
<li>torch-tensorrt: 推理优化库</li>
</ul>
<h1 id="训练优化实践"><a href="#训练优化实践" class="headerlink" title="训练优化实践"></a>训练优化实践</h1><h2 id="创建示例模型"><a href="#创建示例模型" class="headerlink" title="创建示例模型"></a>创建示例模型</h2><p>当搭建完一个深度学习网络，您可以使用NVIDIA Nsight Systems工具进行基准性能分析，观察哪些地方可以做优化。本文以TensorBoard-plugin tutorial中的示例模型为例，演示如何利用NVIDIA Nsight Systems工具寻找模型优化的机会。操作步骤如下所示。</p>
<p>创建一个如下的main.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nvtx  <span class="comment"># 引入nvtx包，便于在nsight system中观察各个函数的逻辑关系。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torch.profiler</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">transform = T.Compose([</span><br><span class="line">    T.Resize(<span class="number">224</span>),</span><br><span class="line">    T.ToTensor(),</span><br><span class="line">    T.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">model = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>).cuda(device)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().cuda(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data, batch_idx</span>):</span><br><span class="line">    <span class="comment"># 数据传输</span></span><br><span class="line">    nvtx.push_range(<span class="string">&quot;copy data &quot;</span> + <span class="built_in">str</span>(batch_idx), color=<span class="string">&quot;rapids&quot;</span>)</span><br><span class="line">    inputs, labels = data[<span class="number">0</span>].to(device=device), data[<span class="number">1</span>].to(device=device)</span><br><span class="line">    nvtx.pop_range()</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    nvtx.push_range(<span class="string">&quot;forward &quot;</span> + <span class="built_in">str</span>(batch_idx), color=<span class="string">&quot;yellow&quot;</span>)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    nvtx.pop_range()</span><br><span class="line">    <span class="comment"># 后向传播</span></span><br><span class="line">    nvtx.push_range(<span class="string">&quot;backward &quot;</span> + <span class="built_in">str</span>(batch_idx), color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    nvtx.pop_range()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于enumerate(train_loader)无法通过插入nvtx统计数据加载时间，将for循环代码替换为如下等价代码。</span></span><br><span class="line">dl = <span class="built_in">iter</span>(train_loader)</span><br><span class="line">batch_idx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 统计最后3个batch总共持续的时间。</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx == <span class="number">5</span>:</span><br><span class="line">            tet = nvtx.start_range(message=<span class="string">&quot;Total Elapsed Time(3 batchs)&quot;</span>, color=<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">        <span class="comment"># 只观察前面8个batch，在这8个batch中前面5个用于wait和warmup，目标观察后面3个</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx &gt;= <span class="number">8</span>:</span><br><span class="line">            nvtx.end_range(tet)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 数据加载。</span></span><br><span class="line">        nvtx.push_range(<span class="string">&quot;__next__ &quot;</span> + <span class="built_in">str</span>(batch_idx), color=<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">        batch_data = <span class="built_in">next</span>(dl)</span><br><span class="line">        nvtx.pop_range()</span><br><span class="line">        <span class="comment"># batch处理，包括数据传输和GPU计算</span></span><br><span class="line">        nvtx.push_range(<span class="string">&quot;batch &quot;</span> + <span class="built_in">str</span>(batch_idx), color=<span class="string">&quot;cyan&quot;</span>)</span><br><span class="line">        train(batch_data, batch_idx)</span><br><span class="line">        nvtx.pop_range()</span><br><span class="line">        batch_idx += <span class="number">1</span></span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        nvtx.pop_range()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>执行以下命令，会在当前目录下生成名为baseline.nsys-rep的文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">nsys profile \</span><br><span class="line">	-w <span class="literal">true</span> \</span><br><span class="line">	--cuda-memory-usage=<span class="literal">true</span> \</span><br><span class="line">	--python-backtrace=cuda \</span><br><span class="line">	-s cpu \</span><br><span class="line">	-f <span class="literal">true</span> \</span><br><span class="line">	-x <span class="literal">true</span> \</span><br><span class="line">	-o baseline \</span><br><span class="line">	python ./main.py</span><br></pre></td></tr></table></figure>
<p>预期输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Files already downloaded and verified</span><br><span class="line">Generating <span class="string">&#x27;/tmp/nsys-report-6673.qdstrm&#x27;</span></span><br><span class="line">[1/1] [========================100%] baseline.nsys-rep</span><br><span class="line">Generated:</span><br><span class="line">    /root/baseline.nsys-rep</span><br></pre></td></tr></table></figure>
<p>将baseline.nsys-rep文件导入Nsight System UI中，即可对模型进行分析。如下为初次打开的效果图：<br><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615154615.png"><br>本文仅参考最后3个Batch（前3个Batch可能会受到初始化和预热效应的影响，性能评估不准确）。如下所示，放大最后3个Batch的位置。<br><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615154804.png"><br>图中有2个NVTX记录Batch时间，一个是在CUDA Stream（Default Stream）中（图中标号为1），一个是在Python线程中（图中标号为2），两者统计的Batch时间（包括copy data、forward、backward）有很大差别。因此如果调用CUDA API并在GPU上运行核函数，此时以在CUDA Stream中的NVTX为准（也就是标号1的数据），这是因为Batch传输和GPU计算对于CPU来说是异步的，Python线程中统计的可能不太准确。</p>
<p>Batch数据加载和Batch计算（包括将Batch传输到GPU）是重叠的，从上图中可以看到，当进行Batch 6的加载操作时，GPU端仍然在计算Batch 5，所以在后续计算3个Batch的总持续时间时，重叠部分不计算时间。</p>
<p>对Baseline各个阶段的持续时间统计如下：</p>
<table>
<thead>
<tr>
<th>各阶段持续时间</th>
<th>耗时（单位：ms）</th>
</tr>
</thead>
<tbody><tr>
<td>平均数据加载时间（图中标记：<strong>next</strong>）</td>
<td>(37.276 + 35.794 + 35.790) &#x2F; 3 &#x3D; 36.287</td>
</tr>
<tr>
<td>平均数据传输时间（图中折叠未显示出）</td>
<td>(4.39 + 4.363 + 4.317) &#x2F; 3 &#x3D; 4.357</td>
</tr>
<tr>
<td>平均Batch处理时间（包含数据传输、forward、backward）</td>
<td>(38.738 + 37.78 + 37.754) &#x2F; 3 &#x3D; 38.091</td>
</tr>
<tr>
<td>3个Batch总的持续时间（如下图所示，从第5个Batch数据加载到第7个Batch计算完成）</td>
<td>176.878 ms</td>
</tr>
<tr>
<td>平均每秒处理样本数（单位：samples&#x2F;s）</td>
<td>32(batch size) * 3 &#x2F; (176.878 &#x2F; 1000) &#x3D; 542.746</td>
</tr>
</tbody></table>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155250.png"></p>
<h2 id="步骤二：优化和分析模型"><a href="#步骤二：优化和分析模型" class="headerlink" title="步骤二：优化和分析模型"></a>步骤二：优化和分析模型</h2><h3 id="模型优化流程"><a href="#模型优化流程" class="headerlink" title="模型优化流程"></a>模型优化流程</h3><p>对于一个单机的深度学习训练任务，主要分为如下几个阶段：</p>
<p>数据加载（Data Loading）：通常情况下，把数据从Disk（或者其他网络存储系统）加载到主机内存，并对数据做预处理操作（例如，去除噪声值），笼统地称为数据加载阶段。</p>
<p>数据传输（Data Transmission）：将数据从主机内存传输到GPU内存。</p>
<p>训练（Training）：GPU计算单元（CUDA Core、Tensor Core）利用这些数据做训练操作。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/Snipaste_2025-06-15_15-53-54.png"></p>
<p>以下将对每部分做一些案例介绍。</p>
<p>优化方向1：缩短数据加载（Data Loading）时间<br>对于一个训练任务而言，缩短数据加载时间对整个训练任务有很大的帮助，如果数据加载时间大于GPU训练的时间，就有可能造成GPU空闲（等待数据加载）。</p>
<p>如下图所示，在Baseline（不包含任何优化措施）中，Batch与Batch计算之间存在很大的空隙，引起这些空隙的根因在于Batch数据加载时间比较长，GPU需要长时间等待数据加载完成才能处理这个Batch的数据，所以数据加载成为整个训练任务的瓶颈。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155449.png"></p>
<p>PyTorch的DataLoader支持多个Worker（Multi-process）同时工作，加载某一个Mini Batch的数据。修改main.py文件中的train_loader参数，尝试设置8个Worker加载数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_set,num_workers=<span class="number">8</span>, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>重新运行，可以看到Batch与Batch之间已经没有空隙，同时数据加载时间大大缩小，三个Batch加载平均消耗时间为：(2.879 ms + 104.995 us + 180.066 us) &#x2F; 3 &#x3D; 1.055 ms。同时，每秒处理样本数（samples&#x2F;s）为：32 * 3 &#x2F;（120.328 &#x2F; 1000）&#x3D; 797.819（注意数据加载时间已经隐藏在GPU计算中，所以忽略），性能相比Baseline提升（797.819 - 542.746）&#x2F; 542.746 * 100% &#x3D; 47.00%。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155521.png"></p>
<h3 id="优化方向2：缩短数据传输（Data-Transmission）时间"><a href="#优化方向2：缩短数据传输（Data-Transmission）时间" class="headerlink" title="优化方向2：缩短数据传输（Data Transmission）时间"></a>优化方向2：缩短数据传输（Data Transmission）时间</h3><p>在设置8个Worker的基础上，可以继续寻找示例模型训练中可优化的点。在上述的分析中，可以看到数据传输平均时间为4.357 ms，尝试开启Pin Memory来缩短数据传输时间。</p>
<p>在PyTorch中，支持将加载的数据直接存放在Pin Memory中。如下所示，修改main.py文件中的train_loader参数，开启Pin Memory。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_set,num_workers=<span class="number">8</span>, batch_size=<span class="number">32</span>,pin_memory=<span class="literal">True</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>重新运行代码，可以看到数据传输的平均时间为：（1.956 + 1.979 + 1.989）&#x2F; 3 &#x3D; 1.975 ms，相比Baseline时间缩短4.357 ms - 1.975 ms &#x3D; 2.382 ms。总的持续时间缩短为99.535 ms，平均每秒处理样本数（samples&#x2F;s）为：</p>
<p>32 * 3 &#x2F;(99.535 &#x2F; 1000) &#x3D; 964.485，性能提升（964.485 - 797.819）&#x2F; 797.819 * 100% &#x3D; 20.89%</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155553.png"></p>
<h3 id="优化方向3：数据加载和数据计算完全重叠"><a href="#优化方向3：数据加载和数据计算完全重叠" class="headerlink" title="优化方向3：数据加载和数据计算完全重叠"></a>优化方向3：数据加载和数据计算完全重叠</h3><p>在缩短数据传输时间的基础上，继续寻找优化的机会。分析开启Pin Memory的Timeline发现，虽然Batch加载和Batch计算（包括Batch传输）有重叠效果，但是Batch加载却只能重叠一个Batch计算，如下图，Batch5的加载与Batch4的计算重叠，Batch6的加载与Batch5的计算重叠，Batch7的加载与Batch6的计算重叠。并没有出现例如加载Batch6、Batch7与Batch5计算重叠的情况。同时，在Timeline中发现，在数据加载操作的后面，都会出现一次cudaStreamSynchronize，强制CPU与GPU做一次同步操作，然而每次计算Batch时并不需要这个同步操作。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155618.png"></p>
<p>这个同步操作是由如下的这行代码引入的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[<span class="number">0</span>].to(device=device), data[<span class="number">1</span>].to(device=device)</span><br></pre></td></tr></table></figure>
<p>to函数默认行为是：每次将数据从Host端传输到GPU端后，会执行cudaStreamSynchronize进行同步。然而，to函数亦支持非阻塞模式（异步操作），通过添加参数non_blocking&#x3D;True来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[<span class="number">0</span>].to(device=device,non_blocking=<span class="literal">True</span>), data[<span class="number">1</span>].to(device=device,non_blocking=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>采用非阻塞模式后（异步操作），数据传输至GPU的操作被安排至CUDA流队列末尾而不会阻碍CPU，即CPU可继续执行后续任务，无需等待数据传输完成。</p>
<p>修改代码后，重新运行，结果显示如下。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155658.png"></p>
<p>从上面的图中可以看出：</p>
<p>GPU真正在计算Batch3时，CPU端Batch5、Batch6、Batch7数据加载已经完成（也就是CPU早早完成自己的任务），真正做到Batch加载与Batch计算完全重叠。</p>
<p>Batch5、Batch6、Batch7的加载已经完全隐藏在了Batch2、Batch3、Batch4的计算中，它们的加载时间可以忽略不计。</p>
<p>每秒样本处理数（samples&#x2F;s）为：32 * 3 &#x2F; (94.164 &#x2F; 1000) &#x3D; 1019.498。相比上一步的优化2，性能提升（1019.498 - 964.485）&#x2F; 964.485 * 100% &#x3D; 5.7%。</p>
<h3 id="优化方向4：自动混合精度"><a href="#优化方向4：自动混合精度" class="headerlink" title="优化方向4：自动混合精度"></a>优化方向4：自动混合精度</h3><p>前面已经优化了数据加载、数据传输。本次将聚焦寻找一些降低GPU计算Batch时间的方法。</p>
<p>PyTorch支持在训练的时候开启混合精度计算（ Automatic Mixed Precision (AMP)），在AMP模式下，GPU上部分Tensor自动转换为低精度的16位浮点数，并在GPU张量核心上运行，以此降低显存使用量和缩短计算时间。</p>
<p>修改下面的代码开启AMP模式。在生产环境中，AMP的完整实现可能需要梯度缩放，下方代码演示中没有包括这一点，请参考相关文档正确使用AMP。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train step</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data</span>):</span><br><span class="line">    ...... <span class="comment"># 省略其他代码</span></span><br><span class="line">    <span class="comment"># 开启amp</span></span><br><span class="line">    <span class="keyword">with</span> torch.autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">    	outputs = model(inputs)</span><br><span class="line">    	loss = criterion(outputs, labels)</span><br><span class="line">    ....... <span class="comment"># 省略其他代码</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>开启AMP的结果如下。Batch计算时间缩短为58.938 ms，平均每秒处理的样本数为：32 * 3 &#x2F; (58.938 &#x2F;1000) &#x3D; 1628.83，性能提升（1628.83 - 1019.498）&#x2F; 1019.498 * 100% &#x3D; 59.77%。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155758.png"></p>
<p>进入一个Batch的forward和Backward，观察其Kernel函数的组成，发现开启AMP之前，操作的数据类型基本都是float32，并且将Kernel函数执行时间按降序排列，执行时间靠前的如下所示，从300μs到1ms不等。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155855.png"></p>
<p>开启AMP以后，大多数操作的数据类型都是float16，并且执行时间靠前的Kernel基本都在100μs到300μs之间。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615155952.png"></p>
<h3 id="优化方向5：增大Batch-Size"><a href="#优化方向5：增大Batch-Size" class="headerlink" title="优化方向5：增大Batch Size"></a>优化方向5：增大Batch Size</h3><p>增大Batch Size也能够提升每秒处理样本数，但是增大Batch Size需要考虑显存的使用情况（显存是否够用）和大Batch Size对Loss值的影响。</p>
<p>修改如下代码，尝试将Batch Size从32调整至512。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_set,num_workers=<span class="number">8</span>, batch_size=<span class="number">512</span>,pin_memory=<span class="literal">True</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>运行代码，结果如下。总的持续时间为：697.666 ms，平均每秒处理样本数（samples&#x2F;s）为：512 * 3 &#x2F;（697.666 &#x2F; 1000）&#x3D; 2201.627。性能提升（2201.627 - 1628.83）&#x2F; 1628.83 * 100% &#x3D; 35.17%。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615160311.png"></p>
<h3 id="优化方向6：模型编译"><a href="#优化方向6：模型编译" class="headerlink" title="优化方向6：模型编译"></a>优化方向6：模型编译</h3><p>默认情况下，PyTorch采用的是即时编译，您可以借助PyTorch的编译API将模型编译成图模式（Graph Mode）。</p>
<p>修改如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>).cuda(device)</span><br><span class="line">model = torch.<span class="built_in">compile</span>(model)</span><br></pre></td></tr></table></figure>
<p>运行代码，结果显示如下。总的持续时间为：567.971 ms，平均每秒处理的样本数为：512 * 3 &#x2F;（567.971 &#x2F; 1000）&#x3D; 2704.36，性能提升（2704.36 - 2201.627）&#x2F; 2201.627 * 100% &#x3D; 22.83%。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615160357.png"></p>
<h3 id="优化方向7：Batch传输与Batch计算重叠"><a href="#优化方向7：Batch传输与Batch计算重叠" class="headerlink" title="优化方向7：Batch传输与Batch计算重叠"></a>优化方向7：Batch传输与Batch计算重叠</h3><p>由于Batch传输和Batch计算是串行执行的，也就是Batch传输完成，才能执行Batch计算。而Batch传输时，GPU是处于空闲状态的，然后当Batch Size变大时，Batch传输时间相比Batch计算是不可忽略的。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615160429.png"></p>
<p>按照一般的思维，要计算Batch，只有当Batch传输到GPU后，才能开始计算，如果传输没有完成就开始计算，结果就不是正确的。如果以流水线思维思考，假设Batch计算的时间比Batch传输的时间长，那么如果GPU在计算第一个Batch的同时，第二个Batch也开始传输，等GPU计算第二个Batch的同时，第三个Batch传输也在进行，那么就将Batch传输的时间隐藏在Batch的计算中了。</p>
<p>在单个Stream中无法完成上述的重叠操作，不过PyTorch允许创建多个Stream，除了默认的Stream外，可以额外创建一个Stream。完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nvtx <span class="comment"># 引入nvtx包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torch.profiler</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.models</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">transform = T.Compose([</span><br><span class="line">    T.Resize(<span class="number">224</span>),</span><br><span class="line">    T.ToTensor(),</span><br><span class="line">    T.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set,num_workers=<span class="number">8</span>, batch_size=<span class="number">512</span>,pin_memory=<span class="literal">True</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">model = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>).cuda(device)</span><br><span class="line">model = torch.<span class="built_in">compile</span>(model)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss().cuda(device)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, device, train_loader, optimizer,criterion</span>):</span><br><span class="line">	model.train()</span><br><span class="line">	dl = <span class="built_in">iter</span>(train_loader)</span><br><span class="line">	transfered_data = []</span><br><span class="line">	<span class="comment">#  创建一个新的stream</span></span><br><span class="line">	s = torch.cuda.Stream()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">prepare_batch</span>(<span class="params">batch_idx</span>):</span><br><span class="line">		nvtx.push_range(<span class="string">&quot;__next__ &quot;</span> + <span class="built_in">str</span>(batch_idx),color=<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">		(inputs, labels) = <span class="built_in">next</span>(dl)</span><br><span class="line">		<span class="comment"># 传输数据在新的stream中进行</span></span><br><span class="line">		nvtx.pop_range()</span><br><span class="line">		nvtx.push_range(<span class="string">&quot;cpy &quot;</span> + <span class="built_in">str</span>(batch_idx),color=<span class="string">&quot;rapids&quot;</span>)</span><br><span class="line">		<span class="keyword">with</span> torch.cuda.stream(s):</span><br><span class="line">			inputs, labels = inputs.to(device=device,non_blocking=<span class="literal">True</span>), labels.to(device=device,non_blocking=<span class="literal">True</span>)</span><br><span class="line">		nvtx.pop_range()</span><br><span class="line">		<span class="keyword">return</span> (inputs, labels)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">batch_compute</span>(<span class="params">batch_idx,data</span>):</span><br><span class="line">		nvtx.push_range(<span class="string">&quot;batch &quot;</span> + <span class="built_in">str</span>(batch_idx),color=<span class="string">&quot;cyan&quot;</span>)</span><br><span class="line">		nvtx.push_range(<span class="string">&quot;forward &quot;</span> + <span class="built_in">str</span>(batch_idx),color=<span class="string">&quot;yellow&quot;</span>)</span><br><span class="line">		<span class="keyword">with</span> torch.autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">			outputs = model(data[<span class="number">0</span>])</span><br><span class="line">			loss = criterion(outputs, data[<span class="number">1</span>])</span><br><span class="line">		nvtx.pop_range()</span><br><span class="line">		nvtx.push_range(<span class="string">&quot;backward &quot;</span> + <span class="built_in">str</span>(batch_idx),color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">		optimizer.zero_grad()</span><br><span class="line">		loss.backward()</span><br><span class="line">		optimizer.step()</span><br><span class="line">		nvtx.pop_range()</span><br><span class="line">		torch.cuda.current_stream().wait_stream(s)</span><br><span class="line">		nvtx.pop_range()</span><br><span class="line"></span><br><span class="line">	batch_idx = <span class="number">0</span></span><br><span class="line">	tet = <span class="literal">None</span></span><br><span class="line">	<span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">		<span class="keyword">try</span>:</span><br><span class="line">			<span class="keyword">if</span> batch_idx == <span class="number">6</span>:</span><br><span class="line">				tet = nvtx.start_range(message=<span class="string">&quot;Total Elapsed Time(3 batchs)&quot;</span>, color=<span class="string">&quot;orange&quot;</span>)</span><br><span class="line">			<span class="keyword">if</span> batch_idx &gt;= <span class="number">8</span>:</span><br><span class="line">				<span class="keyword">break</span></span><br><span class="line">			data = prepare_batch(batch_idx)</span><br><span class="line">			transfered_data.append(data)</span><br><span class="line">			<span class="comment"># 当batch_idx为0时，不执行计算操作</span></span><br><span class="line">			<span class="comment"># 每一个循环都需要让默认的stream等待新创建的stream传输数据完成，确保下一轮</span></span><br><span class="line">                        <span class="comment"># Batch计算所需数据能够准备完成。</span></span><br><span class="line">			<span class="keyword">if</span> batch_idx &gt; <span class="number">0</span>:</span><br><span class="line">				batch_compute(batch_idx-<span class="number">1</span>,transfered_data[batch_idx - <span class="number">1</span>])</span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				torch.cuda.current_stream().wait_stream(s)</span><br><span class="line">			batch_idx += <span class="number">1</span></span><br><span class="line">		<span class="keyword">except</span> StopIteration:</span><br><span class="line">			nvtx.pop_range()</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">	batch_compute(batch_idx-<span class="number">1</span>,transfered_data[batch_idx - <span class="number">1</span>])</span><br><span class="line">	nvtx.end_range(tet)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train(model, device, train_loader, optimizer,criterion)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行代码，结果展示如下。图中展示了Batch2到Batch7的传输操作隐藏在了Batch1到Batch4的计算中（Batch0和Batch1的传输未展示，页面有限）。同时可以确认的是，每个Batch在计算之前，其数据都已经传输完成。例如，Batch4计算时，Batch4的传输已经隐藏在Batch2的计算中。因此，可以确保计算结果的正确性。</p>
<p><img src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/20250615160504.png"></p>
<p>总的持续时间为484.016 ms，平均每秒处理样本数为：512 * 3 &#x2F; (484.016 &#x2F; 1000) &#x3D; 3173.449，性能提升（3173.449 - 2704.36）&#x2F; 2704.36 * 100% &#x3D; 17.35%。</p>
<p>需要注意的是，虽然将数据传输隐藏在数据计算中，但是会额外增加GPU内存的使用，以上面为例，当计算Batch5时，所有的Batch都驻留在了GPU内存中。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对各个优化项性能提升做一个总结。</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>平均每秒处理样本数（samples&#x2F;s）</th>
<th>性能相比前一个优化项提升百分比</th>
</tr>
</thead>
<tbody><tr>
<td>未优化：Baseline</td>
<td>542.746</td>
<td>-</td>
</tr>
<tr>
<td>优化1：Data Loader worker设置为8</td>
<td>797.819</td>
<td>47.00%</td>
</tr>
<tr>
<td>优化2：开启Pin Memory</td>
<td>964.485</td>
<td>20.89%</td>
</tr>
<tr>
<td>优化3：数据加载与Batch计算完全重叠</td>
<td>1019.498</td>
<td>5.7%</td>
</tr>
<tr>
<td>优化4：自动混合精度</td>
<td>1628.83</td>
<td>59.77%</td>
</tr>
<tr>
<td>优化5：增大Batch Size</td>
<td>2201.627</td>
<td>35.17%</td>
</tr>
<tr>
<td>优化6：模型编译</td>
<td>2704.36</td>
<td>22.83%</td>
</tr>
<tr>
<td>优化7：Batch传输与Batch计算重叠</td>
<td>3173.449</td>
<td>17.35%</td>
</tr>
</tbody></table>
</div><div class="article-licensing box"><div class="licensing-title"><p>模型训练推理优化实践(一):训练优化</p><p><a href="http://www.airchaoz.top/2025/06/15/模型训练推理优化实践/">http://www.airchaoz.top/2025/06/15/模型训练推理优化实践/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>echo</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-06-15</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-06-15</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/07/14/%E6%A8%A1%E6%9D%BF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">C++模板学习笔记（一）</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/06/05/Transformer%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8/"><span class="level-item">Transformer模型入门</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://www.airchaoz.top/2025/06/15/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/';
            this.page.identifier = '2025/06/15/模型训练推理优化实践/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'echos-blog-2' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://blog-512.oss-cn-shenzhen.aliyuncs.com/202401032048869.png" alt="echo"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">echo</p><p class="is-size-6 is-block">C++ Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>GuangDong, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">17</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CMake/"><span class="tag">CMake</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Qt/"><span class="tag">Qt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"><span class="tag">内存模型</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"><span class="tag">并发编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"><span class="tag">并行计算</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"><span class="tag">开发工具</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"><span class="tag">性能优化</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%86%85%E5%AD%98/"><span class="tag">操作系统, 内存</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7/"><span class="tag">效率工具</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><span class="tag">源码阅读</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C-%E6%8A%93%E5%8C%85/"><span class="tag">网络, 抓包</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"><span class="tag">网络编程</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91/"><span class="tag">跨平台开发</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#环境准备"><span class="level-left"><span class="level-item">1</span><span class="level-item">环境准备</span></span></a></li><li><a class="level is-mobile" href="#训练优化实践"><span class="level-left"><span class="level-item">2</span><span class="level-item">训练优化实践</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#创建示例模型"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">创建示例模型</span></span></a></li><li><a class="level is-mobile" href="#步骤二：优化和分析模型"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">步骤二：优化和分析模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#模型优化流程"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">模型优化流程</span></span></a></li><li><a class="level is-mobile" href="#优化方向2：缩短数据传输（Data-Transmission）时间"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">优化方向2：缩短数据传输（Data Transmission）时间</span></span></a></li><li><a class="level is-mobile" href="#优化方向3：数据加载和数据计算完全重叠"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">优化方向3：数据加载和数据计算完全重叠</span></span></a></li><li><a class="level is-mobile" href="#优化方向4：自动混合精度"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">优化方向4：自动混合精度</span></span></a></li><li><a class="level is-mobile" href="#优化方向5：增大Batch-Size"><span class="level-left"><span class="level-item">2.2.5</span><span class="level-item">优化方向5：增大Batch Size</span></span></a></li><li><a class="level is-mobile" href="#优化方向6：模型编译"><span class="level-left"><span class="level-item">2.2.6</span><span class="level-item">优化方向6：模型编译</span></span></a></li><li><a class="level is-mobile" href="#优化方向7：Batch传输与Batch计算重叠"><span class="level-left"><span class="level-item">2.2.7</span><span class="level-item">优化方向7：Batch传输与Batch计算重叠</span></span></a></li><li><a class="level is-mobile" href="#总结"><span class="level-left"><span class="level-item">2.2.8</span><span class="level-item">总结</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">echo&#039;s blog</a><p class="is-size-7"><span>&copy; 2025 echo</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>